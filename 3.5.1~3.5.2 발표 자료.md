# 3.5 Alernative Probabilistic Models
 ## 3.5.1 BM25 (Best Match)
 >  전통적인 벡터 모델에서 우수한 용어 가중치는 세 개의 요소 (IDF, TF, doc-length normalization)에 기반한다는 생각에서 출발<br>
그렇기에 IDF 특성만 포함한 전통 확률모델 순위화 함수에 나머지 두 요소 (TF, document length normalization)을 포함하면 더 좋은 결과를 보일 것이라는 가정에 기반<br><br>
Robert-Sparck Jones 수식<br>
![3.17](./image/3.17.png)<br>

### BM1, BM11 and BM15 Ranking Formulas
>1 - TF 요소 계산<br>
![3.35](./image/3.35.png)<br>

>2 - Document 길이 정규화 요소 계산<br>
![3.36](./image/3.36.png)<br>

>3 - Document와 Query크기에 종속적인 수정요소 계산<br>
![3.37](./image/3.37.png)<br>
이 요소는 특정 document(d_j) 에 match된 질의어에만 종속되지 않는 전역 요소가 된다.<br>

>4 - Query내 TF 요소 계산<br>
![3.38](./image/3.38.png)<br>

>위의 요소들을 Robert-Sparck Jones 수식(BM 1) 에 적용하면 초기의 BM1, BM11, BM15 수식이 된다.<br>
>그러나 후의 실험에서 수정 요소(계산식 3)이 필요없음이 밝혀졌고, 균형 상수 S1 = (K1+1), S3 = (K3+1)가 우수한 결과를 보였으며, K3는 비교적 큰 값이 좋았다.<br>
>이 경우 F_i,q는 f_i,q로 단순화 되며, 짧은 쿼리의 경우, f_i,q는 모든 term에 대해 1로 가정하면 전체 수식은 단순해진다.<br>
#### 실제 사용된 순위화 수식
>![3.39_1](./image/3.39_1.png)<br>
>![3.39_2](./image/3.39_2.png)<br>
>![3.39_3](./image/3.39_3.png)<br>
>TREC Data에 대한 실험 결과, BM11이 BM15보다 성능이 우수하다. 즉, 문헌크기 정규화 요소가 중요하다는 결론.

### BM25 Ranking function
>![3.40](./image/3.40.png)<br>
BM25 수식은 TF 요소를 아래와 같이 조정했다.<br>
위 수식에서 K1, b는 수식의 실험 상수이다.<br>
b는 0에서 1사이의 값을 가진다. b값이 0이 된다면 BM15 수식, 1이 된다면 BM11수식, 0 < b < 1 값을 가지면 BM11수식과 BM15수식이 혼합되어 BM25 수식이 된다.<br><br>
>![3.41](./image/3.41.png)<br>
>K1 = 1이 실제 컬렉션에 잘 동작되는 합리적인 가정
>b는 1에 가깝게 하여 문헌 크기 정규화 요소를 강화하는 것이 좋은데, b = 0.75가 적정했다.
> #### * 각 컬렉션에 따라 상수들이 최적값으로 조정돼야 한다.
> #### * BM25 모델은 전통 확률 모델과는 다르게 사용자가 제공하는 연관성 정보 없이 계산될 수 있다.<br>
> #### * 또 일반 컬렉션에서 상수 값들이 적절하게 조정된다면, 전통적인 벡터 모델보다 더 나은 성능을 보여주기에 벡터모델을 대신해 ranking function의 새로운 비교기준이 되었다.

### ES에서 사용하는 BM25 Ranking function
>![3.41_3](./image/3.41_3.png)<br>
> #### * 그러나 결과값이 사용자의 corpus feature에 의존하기 때문에 유의해서 사용할 필요가 있다.<br>

 ## 3.5.2 언어 모델 (Language Models)
  >  document 분포 확률을 정의하여 query term이 나올 가능성을 예측.<br>
즉, query를 사용해 document의 similarity를 예측하지 않고 컬렉션 내 각 document의 언어 모델을 정의하고 주어진 query의 생성 가능성을 예측하며, 이를 확률순으로 정렬하면 document ranking이 가능.

### Statistical foundation (Kalt model)
>   컬렉션 document에 출현하는 연속된 r개의 term들로 구성된 단어의 연속을 S라 하자.<br>
>   즉 ![sequence](./image/sequence.png)가 되며,<br> n-gram 언어 모델은 S의 출현 확률을 구하기 위해 마르코프(markov)모델을 적용한다.<br>
>   ![markov](./image/markov.png)<br>
>   정보 검색의 경우 어순 영향이 명확하지 않아 용어 독립성을 가정해 unigram model이 널리 사용된다.<br>
>   #### * unigram model : 용어 ![ki](./image/ki.png)의 확률 계산에는 단지 ![pki](./image/pki.png)계산만 필요

### Multinomial process (다항 과정)에 기초한 language model
> 다항 과정에 따라 Term들은 서로 독립(unigram model)이라고 가정하여 다음과 같이 정의된다.<br>
![3.42](./image/3.42.png) 이 수식의 양변에 로그를 취하면, <br>
![3.43](./image/3.43.png) <br>
document에 query가 포함되지 않은 분포 때문에, query가 포함되지 않는 전체 document 컬렉션 통계가 필요하다. 즉,<br>
![3.44](./image/3.44.png) 라고 할 수 있다. <br>
> #### * ![alpha](./image/alpha.png)는 document 와 연관된 parameter 이고, ![pkic](./image/pkic.png)는 Collection C에 대해 ![sumpkic](./image/sumpkic.png)와 같이 주어진 언어모델이 될 수 있다.<br>
> 위 수식을 처음의 수식에 적용하면, <br>
![3.45](./image/3.45.png)
> #### * 여기서 nq는 query 크기를 나타내며 마지막 항은 모든 document dj에 대해 상수이므로 제거한다.
> ![3.45_1](./image/3.45_1.png) 이 수식은 document에 출현하는 query term에 weight를 할당함을 뜻한다.<br>
> Term weight는 document내 TF에 비례하며, 벡터 모델의 IDF와 같은 역할을 한다.<br>
> 또한 ![alpha](./image/alpha.png)는 문헌 크기 정규화로 사용될 수 있다.<br>
> #### * query를 생성하기 위한 smoothing 과정이 TF, IDF, doc-len normalization을 포함시키게 했다는 것이 중요하다.<br>

### Smoothing(평탄화)
> 언어모델의 단점은 corpus에 존재하지 않는 term의 경우 그 확률이 0이 되는 것이다. corpus에 의존적이기에 범용적인 모델을 구축하기가 힘들다.<br>
> 또한 조사, 어미 등 stopword가 topic words보다 훨씬 빈도가 높아 원하는 결과를 얻기 쉽지 않을 수 있다. <br>
> smoothing은 이 같은 문제를 극복하기 위한 하나의 기법이다. <br>
> 대표적인 방법은 확률 계산시 확률값 0를 피하기 위해 ![pnotin](./image/pnotin.png)대신 전체 콜렉션 통계인 ![pkic](./image/pkic.png)를 사용하는 것이다. <br>

### Smoothing using Jelinek-Mercer method
> 이 기법은 document 빈도수와 전체 콜렉션 빈도 distribution 사이에 linear interpolation을 사용한다.<br>
> ![smoothing_interpolation](./image/smoothing_interpolation.png)<br>
> 람다 파라미터는 경험적으로 설정된다. 0에 가까울수록 document 빈도수의 영향이 커지며, 1에 가까울수록 콜렉션 빈도수의 영향이 커져서 smoothing효과도 커진다<br>

### Bayesian smoothing using Dirichlet priors
> ![smoothing_bayesian](./image/smoothing_bayesian.png)<br>
> 람다 값이 0에 가까울수록 document 빈도수의 영향이 커지며, 1에 가까울수록 콜렉션 빈도수의 영향이 커져서 smoothing효과도 커진다<br>